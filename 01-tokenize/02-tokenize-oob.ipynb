{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bb50278-7bc0-4730-9d6a-40230c9bb056",
   "metadata": {},
   "source": [
    "## Data Preparation - Tokenize FAQ dataset using out-of-the-box GPT2 default tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a713c4-d76c-48b4-9027-0b35d445ffb7",
   "metadata": {},
   "source": [
    "##### Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80cc89c4-ae12-49a1-af03-4c9264acdb1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install --upgrade jupyter\n",
    "!pip install --upgrade ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c062292-9413-48e0-9004-cca579ae42ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install transformers==4.18.0\n",
    "!pip install datasets==2.9.0\n",
    "!pip install pandas==1.4.1\n",
    "!pip install numpy==1.22.2\n",
    "!pip install torch==1.8.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390d1710-ea24-4e79-b136-c91636e70b09",
   "metadata": {},
   "source": [
    "#### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db6414a8-8031-46b9-81c9-67f82ce25ca0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from transformers import set_seed\n",
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets \n",
    "import logging\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267a1f2a-de81-4d61-b23a-ce493b4cb393",
   "metadata": {},
   "source": [
    "##### Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d76dc78-9c18-425b-950b-39c42d0e90e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c02dbd-755a-4670-9b8f-cb83bd0d0d10",
   "metadata": {},
   "source": [
    "##### Log versions of dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04659b4b-20ad-42ca-836a-c6815dff75e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Using transformers version: 4.18.0]\n",
      "[Using datasets version: 2.9.0]\n",
      "[Using torch version: 1.8.1+cu102]\n",
      "[Using pandas version: 1.4.1]\n",
      "[Using numpy version: 1.22.2]\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'[Using transformers version: {transformers.__version__}]')\n",
    "logger.info(f'[Using datasets version: {datasets.__version__}]')\n",
    "logger.info(f'[Using torch version: {torch.__version__}]')\n",
    "logger.info(f'[Using pandas version: {pd.__version__}]')\n",
    "logger.info(f'[Using numpy version: {np.__version__}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6335e8d7-3392-4f84-8007-af9df7f96807",
   "metadata": {},
   "source": [
    "#### Setup essentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e9dbb64-0b25-409c-ba4a-0eb9848633df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "set_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91152721-65c0-479c-8586-df3500e4ffcc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 512\n",
    "N_GPUS = 1\n",
    "num_proc = int(os.cpu_count()/N_GPUS)\n",
    "num_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "170c3a68-c03a-4b6a-969e-2dffdaec8603",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BOS_TOKEN = '<|startoftext|>'\n",
    "EOS_TOKEN = '<|endoftext|>'\n",
    "PAD_TOKEN = '<|pad|>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22b3738-b0ba-474b-a777-194e9a5a2d1b",
   "metadata": {},
   "source": [
    "#### Load FAQ dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02d2407e-e65a-4f9f-bd0a-70d38e16df05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-bf258d3d4adb68a9\n",
      "Found cached dataset csv (/tmp/cache/csv/default-bf258d3d4adb68a9/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset('csv', \n",
    "                     data_files='./data/faq_train.csv', \n",
    "                     column_names=['question', 'answer'], \n",
    "                     delimiter=',', \n",
    "                     split='train', \n",
    "                     #download_mode='force_redownload',\n",
    "                     cache_dir='/tmp/cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "caa975a1-db18-44b9-8bf8-f9c93bbc68b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded dataset: Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 6787\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'Loaded dataset: {data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37d5e18-db0d-48e3-adaf-a0a94a9d43cf",
   "metadata": {},
   "source": [
    "#### Create data splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03af643f-ef6d-437b-9f63-3be0869090d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /tmp/cache/csv/default-bf258d3d4adb68a9/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d8a4dfe1e0f06e33.arrow and /tmp/cache/csv/default-bf258d3d4adb68a9/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-4346dc9584e850e1.arrow\n",
      "Data splits: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 6108\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 679\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_validation_test = data.train_test_split(shuffle=True, seed=123, test_size=0.1)\n",
    "data_splits = DatasetDict({'train': train_validation_test['train'],  \n",
    "                           'validation': train_validation_test['test']})\n",
    "logger.info(f'Data splits: {data_splits}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106720d9-3b2c-49ac-9a34-55c1feaba736",
   "metadata": {},
   "source": [
    "#### Setup the custom GPT2 tokenizer \n",
    "\n",
    "Re-create the custom tokenizer we created in the previous medium article \"[Easily Build Your Own GPT from Scratch using AWS: A Comprehensive Guide for Domain Adaptation](https://medium.com/@shankar.arunp/easily-build-your-own-gpt-from-scratch-using-aws-51811b6355d3)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cc50d07-990c-4980-b70c-c534b54e7893",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Tokenizer: PreTrainedTokenizer(name_or_path='gpt2', vocab_size=50257, model_max_len=512, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '<|pad|>'})\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', \n",
    "                                          bos_token=BOS_TOKEN,\n",
    "                                          eos_token=EOS_TOKEN, \n",
    "                                          pad_token=PAD_TOKEN, \n",
    "                                          return_tensors='pt')\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.model_max_length = MAX_LEN\n",
    "logger.info(f'Tokenizer: {tokenizer}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd1dacd-f28e-403e-91dc-6ff7684861ef",
   "metadata": {},
   "source": [
    "#### Tokenize data splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9a88a92-f751-4a97-a932-d6b9d21d4944",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(samples: list):\n",
    "    questions = samples['question']\n",
    "    answers = samples['answer']\n",
    "    logger.info(f'Tokenizing QA pairs of length = {len(questions)}')\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    labels = []\n",
    "    \n",
    "    for question, answer in zip(questions, answers):\n",
    "        prompted_input = f'{BOS_TOKEN}question: {question}{PAD_TOKEN}answer: {answer}{EOS_TOKEN}'\n",
    "        tokenized_input = tokenizer(prompted_input, \n",
    "                                    truncation=True, \n",
    "                                    max_length=MAX_LEN, \n",
    "                                    padding='max_length')\n",
    "        input_ids.append(torch.tensor(tokenized_input['input_ids'], dtype=torch.long))\n",
    "        attention_mask.append(torch.tensor(tokenized_input['attention_mask']))\n",
    "        labels.append(torch.tensor(tokenized_input['input_ids']))\n",
    "\n",
    "    return {'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6523da5e-7307-4c49-a8eb-18e43655f158",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /tmp/cache/csv/default-bf258d3d4adb68a9/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8e37e8194e811bfb.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /tmp/cache/csv/default-bf258d3d4adb68a9/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-fe7ab5be4dab5123.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /tmp/cache/csv/default-bf258d3d4adb68a9/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-920772b132e77ae6.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /tmp/cache/csv/default-bf258d3d4adb68a9/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-032d0f8611bb796b.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /tmp/cache/csv/default-bf258d3d4adb68a9/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-3e247381291486ae.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /tmp/cache/csv/default-bf258d3d4adb68a9/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-caf5ea29abd9eba9.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /tmp/cache/csv/default-bf258d3d4adb68a9/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-70628e6613dbab8b.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /tmp/cache/csv/default-bf258d3d4adb68a9/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0c47a90b9a28ceb4.arrow\n",
      "Tokenized data = DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 6108\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 679\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenized_data = data_splits.map(tokenize, batched=True, \n",
    "                                 num_proc=num_proc, \n",
    "                                 #load_from_cache_file=False)\n",
    "                                 remove_columns=['question', 'answer'])\n",
    "tokenized_data.set_format('pt', \n",
    "                          columns=['input_ids', 'attention_mask', 'labels'], \n",
    "                          output_all_columns=True)\n",
    "logger.info(f'Tokenized data = {tokenized_data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9feef55-17dc-4222-b153-887028077acb",
   "metadata": {},
   "source": [
    "#### Save tokenized data splits to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21e68895-5bae-4272-869a-84705ab04b73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00738835334777832,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Saving the dataset (0/1 shards)",
       "rate": null,
       "total": 6108,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4146f4e13c1f4c2893ffa32176daac5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/6108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0067255496978759766,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Saving the dataset (0/1 shards)",
       "rate": null,
       "total": 679,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5634c32a3b9144bbb3f69b99ebfe9154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/679 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data.save_to_disk('./data/tokenized-oob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23519b99-670c-46e1-9d8c-44ac6e0da88e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.10 Python 3.8 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.10-gpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
