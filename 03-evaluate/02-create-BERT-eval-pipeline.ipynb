{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bf45dc5-f7cd-4fe6-bd0d-ff63e2ba945f",
   "metadata": {},
   "source": [
    "## Finetune OOB BERT-base model for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a36c84f-9dbb-40cc-8b60-88f5c1a080d7",
   "metadata": {},
   "source": [
    "##### Prerequisites "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722a1b7a-eabf-4d17-a38a-e021f6063311",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "!pip install jupyter==1.0.0\n",
    "!pip install ipywidgets==8.0.4\n",
    "\n",
    "!pip install transformers==4.18.0\n",
    "!pip install datasets==2.9.0\n",
    "!pip install torch==1.10.2+cu113"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531614bc-6e64-4e66-8bd4-f8c0d68ac920",
   "metadata": {},
   "source": [
    "#### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75316444-07df-452d-b44f-348d5d6272eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from transformers import TrainingArguments\n",
    "from transformers import BertTokenizerFast\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "from transformers import set_seed\n",
    "from transformers import Trainer\n",
    "from datasets import DatasetDict\n",
    "import transformers\n",
    "import numpy as np\n",
    "import datasets\n",
    "import logging \n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b9da80-8541-427a-a63c-03484e5365c6",
   "metadata": {},
   "source": [
    "##### Setup logging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a09cf4e6-fe1d-4c7d-a1dd-81351af4baec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4a71a7-b1f3-419b-aa93-1329a27ef4e7",
   "metadata": {},
   "source": [
    "##### Log versions of dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab4435e5-028d-45d2-9bac-346538294961",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Using transformers version: 4.18.0]\n",
      "[Using datasets version: 2.9.0]\n",
      "[Using torch version: 1.10.2+cu113]\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'[Using transformers version: {transformers.__version__}]')\n",
    "logger.info(f'[Using datasets version: {datasets.__version__}]')\n",
    "logger.info(f'[Using torch version: {torch.__version__}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aab5c3-9b4c-4efa-a62c-ef6a05939986",
   "metadata": {},
   "source": [
    "#### Setup essentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bd5f76e-59bd-400c-aaa8-a7349fc62e48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_GPUS = 1\n",
    "TRAIN_EPOCHS = 2\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "EVAL_BATCH_SIZE = 8\n",
    "MAX_LEN = 128\n",
    "LOGGING_STEPS = 64\n",
    "SAVE_STEPS = 10240  # reduce it to a smaler value like 512 if you want to save checkpoints\n",
    "SAVE_TOTAL_LIMIT = 2\n",
    "\n",
    "set_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fdc577-c6ec-4420-bb5e-08016558ddb9",
   "metadata": {},
   "source": [
    "#### Load BERT base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "557c32f8-b6f8-46e8-a524-1914ee794e60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008975028991699219,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 570,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "663dbc94a23a4e4d8d9399e669426899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00662994384765625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 440473133,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2228b95ffc9a4894a0fa72a9e2ceda17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2,  force_download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4640be34-1d51-456d-9453-a7af52d194f5",
   "metadata": {},
   "source": [
    "#### Load OOB BERT tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3ddcfa5-bf25-4503-ad74-b66ec8079d7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizer: PreTrainedTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "logger.info(f'Tokenizer: {tokenizer}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d23f55-1fee-42f8-b029-bfd45956d7b9",
   "metadata": {},
   "source": [
    "#### Tokenize classification dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54c96b2d-fea0-48ce-b6b2-12531ac8b011",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-39044f29c104f4ff\n",
      "Found cached dataset csv (/tmp/cache/csv/default-39044f29c104f4ff/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "Loaded data: Dataset({\n",
      "    features: ['response', 'label'],\n",
      "    num_rows: 1343\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset('csv', \n",
    "                     data_files='./data/clf_dataset.csv', \n",
    "                     delimiter=',', \n",
    "                     split='train', \n",
    "                     cache_dir='/tmp/cache')\n",
    "logger.info(f'Loaded data: {data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486bae9-0f12-4c57-8abc-baa017429932",
   "metadata": {},
   "source": [
    "##### Create data splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27ce5031-2585-40f2-90f4-5abe9f8dd7a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /tmp/cache/csv/default-39044f29c104f4ff/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ed98a41f56082d07.arrow and /tmp/cache/csv/default-39044f29c104f4ff/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-460959e44de30d4d.arrow\n",
      "Data splits: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['response', 'label'],\n",
      "        num_rows: 1208\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['response', 'label'],\n",
      "        num_rows: 135\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_validation_test = data.train_test_split(shuffle=True, seed=123, test_size=0.1)\n",
    "data_splits = DatasetDict({'train': train_validation_test['train'],  \n",
    "                           'validation': train_validation_test['test']})\n",
    "logger.info(f'Data splits: {data_splits}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e04a0b68-1c2c-486b-9482-d29447fc83fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['response'], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c55f00-ddb3-47fd-bab2-50422da347df",
   "metadata": {},
   "source": [
    "##### Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65a6b024-8411-460b-9f5a-87d803a9de1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total number of processes = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /tmp/cache/csv/default-39044f29c104f4ff/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8108d45f5a64ab26.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /tmp/cache/csv/default-39044f29c104f4ff/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a0badf16af05891e.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /tmp/cache/csv/default-39044f29c104f4ff/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f41e83dce88555db.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /tmp/cache/csv/default-39044f29c104f4ff/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1824e501e57253df.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /tmp/cache/csv/default-39044f29c104f4ff/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0dff67b18a3d0365.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /tmp/cache/csv/default-39044f29c104f4ff/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-168c7699a0fd2db9.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /tmp/cache/csv/default-39044f29c104f4ff/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-64aee7158e98e85e.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /tmp/cache/csv/default-39044f29c104f4ff/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f6cb8ad7392cd6ca.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /tmp/cache/csv/default-39044f29c104f4ff/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f8c1bf4a34e15e82.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /tmp/cache/csv/default-39044f29c104f4ff/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-cef96a60e04705a5.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /tmp/cache/csv/default-39044f29c104f4ff/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-557486df23a29dd1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /tmp/cache/csv/default-39044f29c104f4ff/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-4698d922a96e6687.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /tmp/cache/csv/default-39044f29c104f4ff/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-75aaff94f57aebcb.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /tmp/cache/csv/default-39044f29c104f4ff/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-55e1f87166b1c73e.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /tmp/cache/csv/default-39044f29c104f4ff/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-4d23b9c44e5619ab.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /tmp/cache/csv/default-39044f29c104f4ff/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f934398fed8bf0d7.arrow\n",
      "Tokenized data: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['response', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 1208\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['response', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 135\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Tokenize datasets \n",
    "num_proc = int(os.cpu_count()/N_GPUS)\n",
    "logger.info(f'Total number of processes = {num_proc}')\n",
    "tokenized_data = data_splits.map(preprocess_function, batched=True, num_proc=num_proc)\n",
    "logger.info(f'Tokenized data: {tokenized_data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6387be-68c6-4baa-94e1-f019b0351ff9",
   "metadata": {},
   "source": [
    "#### Finetune "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc873c1b-f37c-4aef-b245-7c7464f49bbc",
   "metadata": {},
   "source": [
    "##### Define training hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f70f79f-2d01-4f8b-846c-70d21be2fcf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir='./model', \n",
    "                                  overwrite_output_dir=True, \n",
    "                                  num_train_epochs=TRAIN_EPOCHS,  \n",
    "                                  optim='adamw_torch', \n",
    "                                  save_strategy='steps', \n",
    "                                  evaluation_strategy='epoch',\n",
    "                                  per_device_train_batch_size=TRAIN_BATCH_SIZE, \n",
    "                                  per_device_eval_batch_size=EVAL_BATCH_SIZE, \n",
    "                                  warmup_steps=10, \n",
    "                                  weight_decay=0.01,\n",
    "                                  logging_steps=LOGGING_STEPS,\n",
    "                                  save_steps=SAVE_STEPS, \n",
    "                                  save_total_limit=SAVE_TOTAL_LIMIT,\n",
    "                                  logging_dir='logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773f716f-929a-4bff-8a4b-154d2b3e2167",
   "metadata": {},
   "source": [
    "##### Kick-off training\n",
    "Note: Since we are using a small dataset for finetuning. The model overfits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52de7bdd-4fd6-4b5c-8f91-087408c8287d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: response. If response are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1208\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-02-08 00:06:03.763: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:47] Found unsupported HuggingFace version 4.18.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.16.2']\n",
      "[2023-02-08 00:06:03.806 pytorch-1-10-gpu-p-ml-g4dn-2xlarge-0431c88e252693110a51644c6a08:16617 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-02-08 00:06:03.938 pytorch-1-10-gpu-p-ml-g4dn-2xlarge-0431c88e252693110a51644c6a08:16617 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220304-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220304-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='302' max='302' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [302/302 01:05, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.695800</td>\n",
       "      <td>0.672331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.659000</td>\n",
       "      <td>0.698973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: response. If response are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 135\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: response. If response are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 135\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=302, training_loss=0.6825174053773185, metrics={'train_runtime': 67.1975, 'train_samples_per_second': 35.954, 'train_steps_per_second': 4.494, 'total_flos': 164982965041440.0, 'train_loss': 0.6825174053773185, 'epoch': 2.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(model=model, \n",
    "                  args=training_args, \n",
    "                  train_dataset=tokenized_data['train'], \n",
    "                  eval_dataset=tokenized_data['validation'], \n",
    "                  tokenizer=tokenizer)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cc9a55-3273-4c95-ab38-f3354b45b0fa",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cd53a10-2a76-40c3-bc86-988ab243e66d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./model\n",
      "Configuration saved in ./model/config.json\n",
      "Model weights saved in ./model/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/tokenizer_config.json\n",
      "Special tokens file saved in ./model/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('./model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ffa2eb-44f8-4297-9999-ae87b45b66b0",
   "metadata": {},
   "source": [
    "#### Create a classification pipeline using the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "377dff3d-d766-4fda-939c-a3ddfb9ca880",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./model\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file ./model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./model\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ./model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Didn't find file ./model/added_tokens.json. We won't load it.\n",
      "loading file ./model/vocab.txt\n",
      "loading file ./model/tokenizer.json\n",
      "loading file None\n",
      "loading file ./model/special_tokens_map.json\n",
      "loading file ./model/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "clf = pipeline('sentiment-analysis', \n",
    "               model='./model', \n",
    "               return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9ec2368-b8a9-45ab-a9ae-95162a908929",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.4401438236236572}, {'label': 'LABEL_1', 'score': 0.5598561763763428}]]\n",
      "INFO:sagemaker:[[{'label': 'LABEL_0', 'score': 0.4401438236236572}, {'label': 'LABEL_1', 'score': 0.5598561763763428}]]\n"
     ]
    }
   ],
   "source": [
    "prediction = clf('yes. however if you have a medical condition that precludes your ability to work from an office setting or facility (such as asthma), please consult with local health officials')\n",
    "logger.info(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e441701-4b29-4325-b6b1-b33cf9e4bc3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.8407505750656128}, {'label': 'LABEL_1', 'score': 0.1592494398355484}]]\n",
      "INFO:sagemaker:[[{'label': 'LABEL_0', 'score': 0.8407505750656128}, {'label': 'LABEL_1', 'score': 0.1592494398355484}]]\n"
     ]
    }
   ],
   "source": [
    "prediction = clf('the state of emergency declared in s')\n",
    "logger.info(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead38346-c0dc-4bd6-a0f6-74c3bd61372a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.10 Python 3.8 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.10-gpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
