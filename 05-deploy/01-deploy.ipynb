{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2892202b-89c2-453f-88fe-2a960f858542",
   "metadata": {},
   "source": [
    "##### Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a1a1ff-3434-4987-b0be-b14b85e1c21c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "!pip install transformers==4.26.0\n",
    "!pip install sagemaker==2.100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9588043e-5c7e-44f2-9c45-ee0ccab2f644",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "430dc587-a8f2-424e-86ce-cb568a068a38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker import get_execution_role\n",
    "from transformers import pipeline\n",
    "from sagemaker import Session\n",
    "import transformers\n",
    "import sagemaker\n",
    "import logging\n",
    "import tarfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3e7f25-8600-4b93-a550-52fd9660b275",
   "metadata": {},
   "source": [
    "#### Setup logging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e80cf7d-6634-4029-9d57-983742e08216",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af235019-8358-45f5-8e1f-a6e87481f84f",
   "metadata": {},
   "source": [
    "#### Log versions of dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb23d59f-b624-4606-8cd1-9783c3b86592",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Using transformers: 4.26.0]\n",
      "[Using sageMaker: 2.100.0]\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'[Using transformers: {transformers.__version__}]')\n",
    "logger.info(f'[Using sageMaker: {sagemaker.__version__}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac4422e-756d-42fa-bf40-0b55a3e56811",
   "metadata": {},
   "source": [
    "#### Setup essentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eece5c9e-1f62-48d6-9e66-c09732859606",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = Session()\n",
    "ROLE = get_execution_role()\n",
    "S3_BUCKET = session.default_bucket()\n",
    "INSTANCE_TYPE = 'ml.c5.2xlarge'\n",
    "INSTANCE_COUNT = 2\n",
    "TRANSFORMERS_VERSION = '4.17.0'\n",
    "PYTORCH_VERSION = '1.10.2'\n",
    "PYTHON_VERSION = 'py38'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3193e2-d3b7-4740-aa4b-9a9233039a02",
   "metadata": {},
   "source": [
    "#### Load PPO optimized model from previous module as a HF Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f44095af-ebae-4953-89b3-e4f628078d10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../04-ppo/model/gpt2-ppo-bertscore were not used when initializing GPT2LMHeadModel: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "gpt2_ppo_pipeline = pipeline('text-generation', model='../04-ppo/model/gpt2-ppo-bertscore', clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6382a2b6-849d-4a98-95fd-caf76cbb1476",
   "metadata": {},
   "source": [
    "#### Save pipeline to local dir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17c9b635-8791-43ef-956a-9c4c4b418906",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt2_ppo_pipeline.save_pretrained('./pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80950f34-bc33-4d55-94af-59517c8edcfc",
   "metadata": {},
   "source": [
    "#### Archive saved pipeline artifacts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bf190df-1bdc-4c9e-b2f6-97f58ba397e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_file_paths(directory: str) -> list:\n",
    "        file_paths = [] \n",
    "        for root, directories, files in os.walk(directory):\n",
    "            for file_name in files:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                file_paths.append(file_path)  \n",
    "        return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3765ad0e-73e7-4c79-9481-a2dd756e9acf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tar_artifacts(local_artifacts_path: str, tar_save_path: str, tar_name: str) -> None:\n",
    "        if not os.path.exists(tar_save_path):\n",
    "            os.makedirs(tar_save_path, exist_ok=True)\n",
    "        tar = tarfile.open(f'{tar_save_path}/{tar_name}', 'w:gz')\n",
    "        file_paths = get_file_paths(local_artifacts_path)  \n",
    "        for file_path in file_paths:\n",
    "            file_ = file_path.split('/')[-1]\n",
    "            try:\n",
    "                tar.add(file_path, arcname=file_) \n",
    "            except OSError:\n",
    "                logger.info('Ignoring OSErrors during tar creation.')\n",
    "        tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a57727ee-7405-4e39-9f1e-511fa8f9c2cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tar_artifacts('./pipeline/', '.', 'gpt2-ppo-pipeline.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32e7a5e-8b49-4d0c-b6ac-1957344c768c",
   "metadata": {},
   "source": [
    "#### Copy pipeline archive form local to S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd71697f-da29-4e0e-bd80-5471e2d0d3a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./gpt2-ppo-pipeline.tar.gz to s3://sagemaker-us-east-1-119174016168/model/ppo-pipeline/gpt2-ppo-pipeline.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp gpt2-ppo-pipeline.tar.gz s3://sagemaker-us-east-1-119174016168/model/ppo-pipeline/gpt2-ppo-pipeline.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323545cd-9fb2-4bd8-add7-f24bc5309031",
   "metadata": {},
   "source": [
    "#### Deploy GPT2 PPO pipeline as a SageMaker endpoint for real-time inference\n",
    "Note: You can either deploy the saved GPT2 PPO model we created in the previous module (04-ppo) or deploy the pipeline we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67f2153a-c5f9-477d-8aec-ff656445c4ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODEL_TAR_PATH = 'model/model.tar.gz'\n",
    "MODEL_TAR_PATH = 'model/ppo-pipeline/gpt2-ppo-pipeline.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94c370c6-a194-416d-ab89-31a6372967b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "huggingface_model = HuggingFaceModel(model_data=f's3://{S3_BUCKET}/{MODEL_TAR_PATH}', \n",
    "                                     role=ROLE,\n",
    "                                     transformers_version=TRANSFORMERS_VERSION, \n",
    "                                     pytorch_version=PYTORCH_VERSION,\n",
    "                                     py_version=PYTHON_VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ffb6f4f-e4aa-4c08-9a2b-db056741b95e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating model with name: huggingface-pytorch-inference-2023-02-10-17-36-31-386\n",
      "CreateModel request: {\n",
      "    \"ModelName\": \"huggingface-pytorch-inference-2023-02-10-17-36-31-386\",\n",
      "    \"ExecutionRoleArn\": \"arn:aws:iam::119174016168:role/service-role/AmazonSageMaker-ExecutionRole-20211014T093628\",\n",
      "    \"PrimaryContainer\": {\n",
      "        \"Image\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:1.10.2-transformers4.17.0-cpu-py38-ubuntu20.04\",\n",
      "        \"Environment\": {\n",
      "            \"SAGEMAKER_PROGRAM\": \"\",\n",
      "            \"SAGEMAKER_SUBMIT_DIRECTORY\": \"\",\n",
      "            \"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"20\",\n",
      "            \"SAGEMAKER_REGION\": \"us-east-1\"\n",
      "        },\n",
      "        \"ModelDataUrl\": \"s3://sagemaker-us-east-1-119174016168/model/ppo-pipeline/gpt2-ppo-pipeline.tar.gz\"\n",
      "    }\n",
      "}\n",
      "Creating endpoint-config with name huggingface-pytorch-inference-2023-02-10-17-36-31-991\n",
      "Creating endpoint with name huggingface-pytorch-inference-2023-02-10-17-36-31-991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "predictor = huggingface_model.deploy(initial_instance_count=INSTANCE_COUNT, \n",
    "                                     instance_type=INSTANCE_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f090d95-ca5b-4c93-b002-b0b7f5185d9b",
   "metadata": {},
   "source": [
    "#### Invoke endpoint for inference and perform answer engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f74da892-1f6c-4dfa-b2a9-f02fc2253e60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = {'inputs': 'can covid spread through water?'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4a02167e-bc24-4b08-b4e4-1faee535b20b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_answer(response: list) -> str:\n",
    "    text = response[0]['generated_text']\n",
    "    ans = text.split('answer: ')[-1]\n",
    "    ans = ans.capitalize()\n",
    "    \n",
    "    contains_incomplete_sent = True\n",
    "    if ans.endswith('.'):\n",
    "        contains_incomplete_sent = False\n",
    "    \n",
    "    sents = ans.split('. ')\n",
    "    if contains_incomplete_sent:\n",
    "        sents.pop()\n",
    "        \n",
    "    cleaned_ans = '. '.join(sents).strip()\n",
    "    cleaned_ans = cleaned_ans + '.'\n",
    "    return cleaned_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8694a9e0-9451-4e79-ba14-73c7a55ac34c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Currently there is no scientific evidence to suggest that covid-19 has been transmitted through chlorinated water.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = predictor.predict(data)\n",
    "ans = extract_answer(response)\n",
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bfd238-1e5a-4226-889f-fdc99cd6a19b",
   "metadata": {},
   "source": [
    "#### Delete endpoint (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0235e21c-83fe-4720-b4a6-2e04f892a24b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1407397f-99ce-44d0-badb-ffc8c9034cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.10 Python 3.8 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.10-gpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
